{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7315596c-3adf-45e5-b560-fc40e98d6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install install-jdk\n",
    "!pip install pyspark --user\n",
    "!pip install findspark --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b18f2-67d7-453b-b503-d74196f13d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jdk\n",
    "jdk.install('11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605f051-b46b-4807-a565-33b8aad2f7b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/home/jovyan/.jdk/jdk-11.0.18+10\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.8\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3.8\"\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName('sparktest').setMaster('k8s://https://kubernetes.default.svc:443')\n",
    "conf.set(\"spark.kubernetes.namespace\", \"kubeflow-user-example-com\")\n",
    "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"default-editor\")\n",
    "\n",
    "conf.set(\"spark.kubernetes.container.image\", \"lexcc/spark-py\")\n",
    "conf.set(\"spark.kubernetes.executor.container.image\", \"lexcc/spark-py\")\n",
    "conf.set(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "\n",
    "conf.set(\"spark.kubernetes.allocation.batch.size\", \"5\")\n",
    "conf.set(\"spark.kubernetes.executor.instances\", \"1\")\n",
    "conf.set(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "conf.set(\"spark.driver.host\", \"jupyter\")\n",
    "conf.set(\"spark.driver.port\", \"37371\")\n",
    "conf.set(\"spark.blockManager.port\", \"6060\")\n",
    "\n",
    "conf.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.2')\n",
    "ACCESS_KEY = os.environ.get('ACCESS_KEY')\n",
    "SECRET_KEY = os.environ.get('SECRET_KEY')\n",
    "S3_ENDPOINT = os.environ.get('S3_ENDPOINT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99758e7b-1018-44bf-a5aa-22363d5dbcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).appName(\"Word Count\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02283a99-3a33-4833-bbd8-29ef227451f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_s3_connection(access_key=None, secret_key=None, s3_endpoint=None):\n",
    "    global ACCESS_KEY, SECRET_KEY, S3_ENDPOINT\n",
    "    if access_key is not None:\n",
    "        ACCESS_KEY = access_key\n",
    "    if secret_key is not None:\n",
    "        SECRET_KEY = secret_key\n",
    "    if s3_endpoint is not None:\n",
    "        S3_ENDPOINT = s3_endpoint\n",
    "    if ACCESS_KEY is None or SECRET_KEY is None or S3_ENDPOINT is None:\n",
    "        raise Exception(\"Missing information, S3 connection can't been set\") \n",
    "\n",
    "setup_s3_connection(\"\") # Fill your settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d82f2e-e06a-41b1-8deb-39cf188dc0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", ACCESS_KEY)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", SECRET_KEY)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", S3_ENDPOINT)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3a.buckets.create.enabled\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3n.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3.S3FileSystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f26fd-1fdd-4f4b-9d07-39795b08f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def connect_to_s3():\n",
    "    session = boto3.Session(\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY,\n",
    "    )\n",
    "    conn = session.resource(\"s3\", endpoint_url=S3_ENDPOINT)\n",
    "    return conn\n",
    "\n",
    "def create_bucket(s3, bucket_name):\n",
    "    s3.create_bucket(Bucket=bucket_name)\n",
    "    return\n",
    "\n",
    "def create_bucket_if_not_existed(s3, bucket_name):\n",
    "    if bucket_exist(s3, bucket_name):\n",
    "        return\n",
    "    create_bucket(s3, bucket_name)\n",
    "    return\n",
    "\n",
    "def bucket_exist(s3, bucket_name):\n",
    "    try:\n",
    "        s3.meta.client.head_bucket(Bucket=bucket_name)\n",
    "    except ClientError:\n",
    "        print(\"The bucket does not exist or you have no access.\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def print_file_name_in_bucket(s3, bucket_name):\n",
    "    my_bucket = s3.Bucket(bucket_name)\n",
    "    for file in my_bucket.objects.all():\n",
    "        print(file.key)\n",
    "    return\n",
    "\n",
    "def get_file_from_bucket(s3, bucket_name, file_name, content=None):\n",
    "    try:\n",
    "        obj = s3.Object(bucket_name, file_name)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            print(\"File not found\")\n",
    "            return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    if content is None:\n",
    "        return obj\n",
    "    return obj.get()['Body']\n",
    "\n",
    "def upload_file_to_bucket(s3, bucket_name, file_name, saved_name=None):\n",
    "    if saved_name is None:\n",
    "        saved_name = file_name\n",
    "    s3.meta.client.upload_file(file_name, bucket_name, saved_name)\n",
    "    return\n",
    "\n",
    "def delete_bucket(s3, bucket_name, force):\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    if force:\n",
    "        bucket.objects.all().delete()\n",
    "    bucket.delete()\n",
    "    return\n",
    "\n",
    "s3 = connect_to_s3()\n",
    "bucket_name = \"test\"\n",
    "file = \"train.csv\"\n",
    "create_bucket(s3, bucket_name)\n",
    "upload_file_to_bucket(s3, bucket_name, file)\n",
    "print_file_name_in_bucket(s3, bucket_name)\n",
    "#print(get_file_from_bucket(s3, bucket_name, file, True).read())\n",
    "delete_bucket(s3, bucket_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21377059-f2fc-4f1f-addb-b3f5565b8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_p = pd.read_csv('./train.csv')\n",
    "df_p.FireplaceQu = df_p.FireplaceQu.astype(str)\n",
    "df_p.Fence = df_p.Fence.astype(str)\n",
    "df_p.MiscFeature = df_p.MiscFeature.astype(str)\n",
    "df_p.BsmtQual = df_p.BsmtQual.astype(str)\n",
    "df_p.BsmtCond = df_p.BsmtCond.astype(str)\n",
    "df_p.BsmtExposure = df_p.BsmtExposure.astype(str)\n",
    "df_p.BsmtFinType1 = df_p.BsmtFinType1.astype(str)\n",
    "df_p.BsmtFinType2 = df_p.BsmtFinType2.astype(str)\n",
    "df_p.Alley = df_p.Alley.astype(str)\n",
    "df_p.GarageType = df_p.GarageType.astype(str)\n",
    "df_p.GarageFinish = df_p.GarageFinish.astype(str)\n",
    "df_p.GarageQual = df_p.GarageQual.astype(str)\n",
    "df_p.GarageCond = df_p.GarageCond.astype(str)\n",
    "\n",
    "df_p = df_p.dropna()\n",
    "\n",
    "bucket_name = \"mlpipeline\"\n",
    "folder_name = \"result\"\n",
    "create_bucket_if_not_existed(s3, bucket_name)\n",
    "df = spark.createDataFrame(df_p)\n",
    "s3_path = 's3a://%s/%s' % (bucket_name, folder_name)\n",
    "df.write.format(\"csv\").option(\"header\", \"true\").save(s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ee03d-902c-4020-b4f0-8392afd34048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.context import SQLContext\n",
    "sqlContext = SQLContext(spark)\n",
    "# Read and convert csv into dataframe\n",
    "df = sqlContext.read.csv(s3_path, header=True)\n",
    "store_in_RDD = True\n",
    "s3_path += \"_new\"\n",
    "\n",
    "if store_in_RDD:\n",
    "    # Get RDD object\n",
    "    df_rdd = df.rdd\n",
    "    #Save object in RDD format\n",
    "    df_rdd.saveAsTextFile(s3_path)\n",
    "else:\n",
    "    # Save object in Standard format\n",
    "    df.write.format(\"csv\").option(\"header\", \"true\").save(s3_path)\n",
    "\n",
    "# Read and convert csv files into dataframe\n",
    "test = sqlContext.read.csv(s3_path, header=True)\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa5bb8-570a-402a-97b3-81c78e2d1df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
